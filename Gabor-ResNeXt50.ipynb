{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import numpy as np  \n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers, activations\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,ZeroPadding2D,BatchNormalization,Activation,Add,Dot,AveragePooling2D,Lambda,Reshape\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,DepthwiseConv2D  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'UCM'\n",
    "base_path='./Dataset/' + dataset\n",
    "train_path=os.path.join(base_path, 'Train')\n",
    "ModelName = \"Gabor-ResNeXt50\"\n",
    "\n",
    "All_Labels = os.listdir(train_path)\n",
    "num_classes = len(All_Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "class WarmupExponentialDecay(Callback):\n",
    "    def __init__(self,lr_base=0.0002,lr_min=0.0,decay=0,warmup_epochs=0):\n",
    "        self.num_passed_batchs = 0   #一个计数器\n",
    "        self.warmup_epochs=warmup_epochs  \n",
    "        self.lr=lr_base #learning_rate_base\n",
    "        self.lr_min=lr_min #最小的起始学习率,此代码尚未实现\n",
    "        self.decay=decay  #指数衰减率\n",
    "        self.steps_per_epoch=0 #也是一个计数器\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # params是模型自动传递给Callback的一些参数\n",
    "        if self.steps_per_epoch==0:\n",
    "            #防止跑验证集的时候呗更改了\n",
    "            if self.params['steps'] == None:\n",
    "                self.steps_per_epoch = np.ceil(1. * self.params['samples'] / self.params['batch_size'])\n",
    "            else:\n",
    "                self.steps_per_epoch = self.params['steps']\n",
    "        if self.num_passed_batchs < self.steps_per_epoch * self.warmup_epochs:\n",
    "            K.set_value(self.model.optimizer.lr,\n",
    "                        self.lr*(self.num_passed_batchs + 1) / self.steps_per_epoch / self.warmup_epochs)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr,\n",
    "                        self.lr*((1-self.decay)**(self.num_passed_batchs-self.steps_per_epoch*self.warmup_epochs)))\n",
    "        self.num_passed_batchs += 1\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "    #用来输出学习率的,可以删除\n",
    "        print(\"learning_rate:\",K.get_value(self.model.optimizer.lr))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def Gabor(shape, dtype=None):    \n",
    "    ksize = (7, 7)\n",
    "    # 核尺寸\n",
    "    #sigmas = [1] # [2, 4]\n",
    "    # 角度\n",
    "    #thetas = np.linspace(0, 2*np.pi, 8, endpoint=False) # np.linspace(0, np.pi, 4, endpoint=False)\n",
    "    # 波长(间隔)\n",
    "    #lambdas = np.linspace(2, 3, 6) # [8, 16, 32, 64]\n",
    "    # 高度(越小，核函数图像会越高)\n",
    "    #gammas = [1] # np.linspace(1, 0, 2, endpoint=False)\n",
    "    # 中轴\n",
    "    #psis = [0, 2*np.pi]\n",
    "    \n",
    "    gabors = []\n",
    "    \n",
    "    for i in range(0,int(64/4)):    \n",
    "#     size, sigma, theta, lambda, gamma aspect ratio                 \n",
    "        gf = cv2.getGaborKernel(ksize=ksize, sigma=1, theta=0, lambd=2, gamma=1, psi=0, ktype=cv2.CV_32F)\n",
    "        gabors.append(gf)             \n",
    "        gf = cv2.getGaborKernel(ksize=ksize, sigma=1, theta=np.pi/2, lambd=2, gamma=1, psi=0, ktype=cv2.CV_32F)\n",
    "        gabors.append(gf)             \n",
    "        gf = cv2.getGaborKernel(ksize=ksize, sigma=1, theta=np.pi/4, lambd=2, gamma=1, psi=0, ktype=cv2.CV_32F)\n",
    "        gabors.append(gf)             \n",
    "        gf = cv2.getGaborKernel(ksize=ksize, sigma=1, theta=np.pi/4*3, lambd=2, gamma=1, psi=0, ktype=cv2.CV_32F)\n",
    "        gabors.append(gf)\n",
    "    stacked_list = np.array([gabors])\n",
    "    stacked_list = np.einsum('hijk->jkhi', stacked_list)\n",
    "    \n",
    "    b = K.constant(stacked_list, dtype='float32')\n",
    "    F_0 = Lambda(lambda x: K.cast(x, dtype='float32'))(b)\n",
    "    return F_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block3(x, filters, kernel_size=3, stride=1, groups=32, conv_shortcut=True, name=None):\n",
    "    bn_axis = 3 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    if conv_shortcut is True:\n",
    "        shortcut = Conv2D((64 // groups) * filters, 1, strides=stride, use_bias=False, name=name + '_0_conv')(x)\n",
    "        shortcut = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    x = Conv2D(filters, 1, use_bias=False, name=name + '_1_conv')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\n",
    "    x = Activation('relu', name=name + '_1_relu')(x)\n",
    "\n",
    "    c = filters // groups\n",
    "    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
    "    x = DepthwiseConv2D(kernel_size, strides=stride, depth_multiplier=c, use_bias=False, name=name + '_2_conv')(x)\n",
    "    x_shape = tf.keras.backend.int_shape(x)[1:-1]\n",
    "    x = Reshape(x_shape + (groups, c, c))(x)\n",
    "    output_shape = x_shape + (groups, c) if tf.keras.backend.backend() == 'theano' else None\n",
    "    x = Lambda(lambda x: sum([x[:, :, :, :, i] for i in range(c)]),\n",
    "                      output_shape=output_shape, name=name + '_2_reduce')(x)\n",
    "    x = Reshape(x_shape + (filters,))(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\n",
    "    x = Activation('relu', name=name + '_2_relu')(x)\n",
    "\n",
    "    x = Conv2D((64 // groups) * filters, 1, use_bias=False, name=name + '_3_conv')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_3_bn')(x)\n",
    "\n",
    "    x = Add(name=name + '_add')([shortcut, x])\n",
    "    x = Activation('relu', name=name + '_out')(x)\n",
    "    return x\n",
    "\n",
    "def stack3(x, filters, blocks, stride1=2, groups=32, name=None):\n",
    "    x = block3(x, filters, stride=stride1, groups=groups, name=name + '_block1')\n",
    "    for i in range(2, blocks + 1):\n",
    "        x = block3(x, filters, groups=groups, conv_shortcut=False, name=name + '_block' + str(i))\n",
    "    return x\n",
    "\n",
    "def stack_fn(x):\n",
    "    x = stack3(x, 128, 3, stride1=1, name='conv2')\n",
    "    x = stack3(x, 256, 4, name='conv3')\n",
    "    x = stack3(x, 512, 6, name='conv4')\n",
    "    x = stack3(x, 1024, 3, name='conv5')\n",
    "    return x\n",
    "\n",
    "from keras.optimizers import SGD  \n",
    "def Gabor_ResNeXt50(shape):\n",
    "    if tf.keras.backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        \n",
    "    img_input = Input(shape=shape)  \n",
    "\n",
    "    bn_axis = 3 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    x = ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\n",
    "    x = Conv2D(64, 7, strides=2, use_bias=False, name='conv1_conv')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1_bn')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = ZeroPadding2D(padding=((3, 3), (3, 3)))(x)\n",
    "    x = Conv2D(64, (7, 7), strides=1, use_bias=False, kernel_initializer=Gabor, name='Gabor')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='Gabor-Batch')(x)\n",
    "    x = Activation('relu', name='Gabor-relu')(x)\n",
    "    \n",
    "    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n",
    "    x = MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n",
    "    x = stack_fn(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='post_bn')(x)\n",
    "    x = Activation('relu', name='post_relu')(x)\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dense(1024, activation='relu',name='fc1024')(x)\n",
    "    x = Dense(512, activation='relu',name='fc512')(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='probs')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=img_input,outputs=x, name=ModelName)  \n",
    "    sgd = SGD(decay=0.0001,momentum=0.9)  \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"Gabor-ResNeXt50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9408        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 118, 118, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Gabor (Conv2D)                  (None, 112, 112, 64) 3136        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Gabor-Batch (BatchNormalization (None, 112, 112, 64) 256         Gabor[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Gabor-relu (Activation)         (None, 112, 112, 64) 0           Gabor-Batch[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           Gabor-relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 128)  0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (DepthwiseC (None, 56, 56, 512)  4608        conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 56, 56, 32, 4 0           conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_reduce (Lambda)  (None, 56, 56, 32, 4 0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 56, 56, 128)  0           conv2_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 128)  512         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 128)  0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16384       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  32768       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  32768       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 128)  0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (DepthwiseC (None, 56, 56, 512)  4608        conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 56, 56, 32, 4 0           conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_reduce (Lambda)  (None, 56, 56, 32, 4 0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 56, 56, 128)  0           conv2_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 128)  512         reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 128)  0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  32768       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  32768       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 128)  0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (DepthwiseC (None, 56, 56, 512)  4608        conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 56, 56, 32, 4 0           conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_reduce (Lambda)  (None, 56, 56, 32, 4 0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 56, 56, 128)  0           conv2_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 128)  512         reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 128)  0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  32768       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 56, 56, 256)  65536       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 56, 56, 256)  1024        conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 56, 56, 256)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 58, 58, 256)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (DepthwiseC (None, 28, 28, 2048) 18432       conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 28, 28, 32, 8 0           conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_reduce (Lambda)  (None, 28, 28, 32, 8 0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 28, 28, 256)  0           conv3_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 256)  1024        reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 256)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131072      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  131072      conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 256)  131072      conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 256)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 256)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (DepthwiseC (None, 28, 28, 2048) 18432       conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 28, 28, 32, 8 0           conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_reduce (Lambda)  (None, 28, 28, 32, 8 0           reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 28, 28, 256)  0           conv3_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 256)  1024        reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 256)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  131072      conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 256)  131072      conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 256)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 256)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (DepthwiseC (None, 28, 28, 2048) 18432       conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 28, 28, 32, 8 0           conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_reduce (Lambda)  (None, 28, 28, 32, 8 0           reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 28, 28, 256)  0           conv3_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 256)  1024        reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 256)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  131072      conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 256)  131072      conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 256)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 256)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (DepthwiseC (None, 28, 28, 2048) 18432       conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 28, 28, 32, 8 0           conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_reduce (Lambda)  (None, 28, 28, 32, 8 0           reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 28, 28, 256)  0           conv3_block4_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 256)  1024        reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 256)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  131072      conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 28, 28, 512)  262144      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 28, 28, 512)  2048        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 28, 28, 512)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 30, 30, 512)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 14, 14, 512)  0           conv4_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 512)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 512)  524288      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 512)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 512)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 14, 14, 512)  0           conv4_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 512)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 512)  524288      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 512)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 512)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 14, 14, 512)  0           conv4_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 512)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 512)  524288      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 512)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 512)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 14, 14, 512)  0           conv4_block4_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 512)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 512)  524288      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 512)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 512)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 14, 14, 512)  0           conv4_block5_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 512)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 512)  524288      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 512)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 512)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (DepthwiseC (None, 14, 14, 8192) 73728       conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 14, 14, 32, 1 0           conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_reduce (Lambda)  (None, 14, 14, 32, 1 0           reshape_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 14, 14, 512)  0           conv4_block6_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 512)  2048        reshape_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 512)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 524288      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 14, 14, 1024) 1048576     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 14, 14, 1024) 4096        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 14, 14, 1024) 0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 16, 16, 1024) 0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (DepthwiseC (None, 7, 7, 32768)  294912      conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 7, 7, 32, 32, 0           conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_reduce (Lambda)  (None, 7, 7, 32, 32) 0           reshape_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 7, 7, 1024)   0           conv5_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 1024)   4096        reshape_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 1024)   0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2097152     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   2097152     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 1024)   2097152     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 1024)   4096        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 1024)   0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 1024)   0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (DepthwiseC (None, 7, 7, 32768)  294912      conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 7, 7, 32, 32, 0           conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_reduce (Lambda)  (None, 7, 7, 32, 32) 0           reshape_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 7, 7, 1024)   0           conv5_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 1024)   4096        reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 1024)   0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   2097152     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 1024)   2097152     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 1024)   4096        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 1024)   0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 1024)   0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (DepthwiseC (None, 7, 7, 32768)  294912      conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, 7, 7, 32, 32, 0           conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_reduce (Lambda)  (None, 7, 7, 32, 32) 0           reshape_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_32 (Reshape)            (None, 7, 7, 1024)   0           conv5_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 1024)   4096        reshape_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 1024)   0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   2097152     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           post_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc1024 (Dense)                  (None, 1024)         2098176     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc512 (Dense)                   (None, 512)          524800      fc1024[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "probs (Dense)                   (None, 21)           10773       fc512[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 25,693,461\n",
      "Trainable params: 25,621,013\n",
      "Non-trainable params: 72,448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Gabor_ResNeXt50([224,224,3])\n",
    "model.load_weights('./models/ResNeXt50.h5', by_name=True)\n",
    "#model.load_weights('./models/2021-02-20/UCM/Gabor-ResNeXt50.h5', by_name=True)\n",
    "model.summary()\n",
    "#model.layers[6].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 21 classes.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/800\n",
      "learning_rate: 0.01\n",
      "84/84 [==============================] - 51s 608ms/step - loss: 3.1055 - accuracy: 0.0512\n",
      "Epoch 2/800\n",
      "learning_rate: 1e-04\n",
      "84/84 [==============================] - 42s 505ms/step - loss: 2.7591 - accuracy: 0.2452\n",
      "Epoch 3/800\n",
      "learning_rate: 0.0002\n",
      "84/84 [==============================] - 43s 513ms/step - loss: 2.0670 - accuracy: 0.5815\n",
      "Epoch 4/800\n",
      "learning_rate: 0.00019966827\n",
      "84/84 [==============================] - 43s 514ms/step - loss: 1.3418 - accuracy: 0.7661\n",
      "Epoch 5/800\n",
      "learning_rate: 0.00019933311\n",
      "84/84 [==============================] - 43s 516ms/step - loss: 0.8540 - accuracy: 0.8458\n",
      "Epoch 6/800\n",
      "learning_rate: 0.0001989985\n",
      "84/84 [==============================] - 43s 517ms/step - loss: 0.5348 - accuracy: 0.9018\n",
      "Epoch 7/800\n",
      "learning_rate: 0.00019866446\n",
      "84/84 [==============================] - 44s 519ms/step - loss: 0.3856 - accuracy: 0.9083\n",
      "Epoch 8/800\n",
      "learning_rate: 0.00019833099\n",
      "84/84 [==============================] - 44s 520ms/step - loss: 0.2810 - accuracy: 0.9446\n",
      "Epoch 9/800\n",
      "learning_rate: 0.00019799807\n",
      "84/84 [==============================] - 44s 521ms/step - loss: 0.2494 - accuracy: 0.9440\n",
      "Epoch 10/800\n",
      "learning_rate: 0.00019766571\n",
      "84/84 [==============================] - 44s 522ms/step - loss: 0.2208 - accuracy: 0.9476\n",
      "Epoch 11/800\n",
      "learning_rate: 0.00019733391\n",
      "84/84 [==============================] - 44s 522ms/step - loss: 0.2161 - accuracy: 0.9440\n",
      "Epoch 12/800\n",
      "learning_rate: 0.00019700265\n",
      "84/84 [==============================] - 44s 524ms/step - loss: 0.1547 - accuracy: 0.9595\n",
      "Epoch 13/800\n",
      "learning_rate: 0.00019667197\n",
      "84/84 [==============================] - 44s 524ms/step - loss: 0.1441 - accuracy: 0.9690\n",
      "Epoch 14/800\n",
      "learning_rate: 0.00019634183\n",
      "84/84 [==============================] - 44s 524ms/step - loss: 0.1279 - accuracy: 0.9750\n",
      "Epoch 15/800\n",
      "learning_rate: 0.00019601225\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.1291 - accuracy: 0.9720\n",
      "Epoch 16/800\n",
      "learning_rate: 0.00019568323\n",
      "84/84 [==============================] - 44s 525ms/step - loss: 0.1129 - accuracy: 0.9756\n",
      "Epoch 17/800\n",
      "learning_rate: 0.00019535475\n",
      "84/84 [==============================] - 44s 524ms/step - loss: 0.0974 - accuracy: 0.9744\n",
      "Epoch 18/800\n",
      "learning_rate: 0.00019502682\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0848 - accuracy: 0.9810\n",
      "Epoch 19/800\n",
      "learning_rate: 0.00019469945\n",
      "84/84 [==============================] - 44s 525ms/step - loss: 0.0851 - accuracy: 0.9792\n",
      "Epoch 20/800\n",
      "learning_rate: 0.00019437262\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0841 - accuracy: 0.9792\n",
      "Epoch 21/800\n",
      "learning_rate: 0.00019404636\n",
      "84/84 [==============================] - 44s 525ms/step - loss: 0.0807 - accuracy: 0.9804\n",
      "Epoch 22/800\n",
      "learning_rate: 0.00019372063\n",
      "84/84 [==============================] - 44s 525ms/step - loss: 0.0848 - accuracy: 0.9756\n",
      "Epoch 23/800\n",
      "learning_rate: 0.00019339545\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0742 - accuracy: 0.9827\n",
      "Epoch 24/800\n",
      "learning_rate: 0.00019307081\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0621 - accuracy: 0.9875\n",
      "Epoch 25/800\n",
      "learning_rate: 0.00019274672\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0601 - accuracy: 0.9845\n",
      "Epoch 26/800\n",
      "learning_rate: 0.00019242318\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0531 - accuracy: 0.9887\n",
      "Epoch 27/800\n",
      "learning_rate: 0.00019210017\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0537 - accuracy: 0.9881\n",
      "Epoch 28/800\n",
      "learning_rate: 0.00019177771\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0520 - accuracy: 0.9893\n",
      "Epoch 29/800\n",
      "learning_rate: 0.0001914558\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0611 - accuracy: 0.9839\n",
      "Epoch 30/800\n",
      "learning_rate: 0.00019113442\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0361 - accuracy: 0.9935\n",
      "Epoch 31/800\n",
      "learning_rate: 0.00019081357\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0486 - accuracy: 0.9869\n",
      "Epoch 32/800\n",
      "learning_rate: 0.00019049327\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0434 - accuracy: 0.9899\n",
      "Epoch 33/800\n",
      "learning_rate: 0.00019017351\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0459 - accuracy: 0.9899\n",
      "Epoch 34/800\n",
      "learning_rate: 0.00018985428\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0376 - accuracy: 0.9899\n",
      "Epoch 35/800\n",
      "learning_rate: 0.0001895356\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0341 - accuracy: 0.9946\n",
      "Epoch 36/800\n",
      "learning_rate: 0.00018921743\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0402 - accuracy: 0.9917\n",
      "Epoch 37/800\n",
      "learning_rate: 0.00018889981\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0410 - accuracy: 0.9881\n",
      "Epoch 38/800\n",
      "learning_rate: 0.00018858272\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0407 - accuracy: 0.9893\n",
      "Epoch 39/800\n",
      "learning_rate: 0.00018826617\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0308 - accuracy: 0.9923\n",
      "Epoch 40/800\n",
      "learning_rate: 0.00018795015\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0442 - accuracy: 0.9905\n",
      "Epoch 41/800\n",
      "learning_rate: 0.00018763465\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0364 - accuracy: 0.9893\n",
      "Epoch 42/800\n",
      "learning_rate: 0.00018731969\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0368 - accuracy: 0.9935\n",
      "Epoch 43/800\n",
      "learning_rate: 0.00018700525\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0273 - accuracy: 0.9952\n",
      "Epoch 44/800\n",
      "learning_rate: 0.00018669134\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0269 - accuracy: 0.9952\n",
      "Epoch 45/800\n",
      "learning_rate: 0.00018637796\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0286 - accuracy: 0.9940\n",
      "Epoch 46/800\n",
      "learning_rate: 0.00018606511\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0352 - accuracy: 0.9911\n",
      "Epoch 47/800\n",
      "learning_rate: 0.00018575278\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0353 - accuracy: 0.9923\n",
      "Epoch 48/800\n",
      "learning_rate: 0.00018544096\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0330 - accuracy: 0.9911\n",
      "Epoch 49/800\n",
      "learning_rate: 0.00018512968\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0331 - accuracy: 0.9911\n",
      "Epoch 50/800\n",
      "learning_rate: 0.00018481893\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0339 - accuracy: 0.9917\n",
      "Epoch 51/800\n",
      "learning_rate: 0.0001845087\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0207 - accuracy: 0.9970\n",
      "Epoch 52/800\n",
      "learning_rate: 0.00018419897\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0279 - accuracy: 0.9923\n",
      "Epoch 53/800\n",
      "learning_rate: 0.00018388977\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0212 - accuracy: 0.9982\n",
      "Epoch 54/800\n",
      "learning_rate: 0.0001835811\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0223 - accuracy: 0.9964\n",
      "Epoch 55/800\n",
      "learning_rate: 0.00018327293\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0232 - accuracy: 0.9952\n",
      "Epoch 56/800\n",
      "learning_rate: 0.00018296529\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0220 - accuracy: 0.9958\n",
      "Epoch 57/800\n",
      "learning_rate: 0.00018265817\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0167 - accuracy: 0.9982\n",
      "Epoch 58/800\n",
      "learning_rate: 0.00018235155\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0244 - accuracy: 0.9940\n",
      "Epoch 59/800\n",
      "learning_rate: 0.00018204546\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0262 - accuracy: 0.9923\n",
      "Epoch 60/800\n",
      "learning_rate: 0.00018173987\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0209 - accuracy: 0.9952\n",
      "Epoch 61/800\n",
      "learning_rate: 0.0001814348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0184 - accuracy: 0.9964\n",
      "Epoch 62/800\n",
      "learning_rate: 0.00018113025\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0217 - accuracy: 0.9958\n",
      "Epoch 63/800\n",
      "learning_rate: 0.0001808262\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0198 - accuracy: 0.9958\n",
      "Epoch 64/800\n",
      "learning_rate: 0.00018052266\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0138 - accuracy: 0.9982\n",
      "Epoch 65/800\n",
      "learning_rate: 0.00018021963\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0149 - accuracy: 0.9976\n",
      "Epoch 66/800\n",
      "learning_rate: 0.00017991712\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0205 - accuracy: 0.9946\n",
      "Epoch 67/800\n",
      "learning_rate: 0.0001796151\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0194 - accuracy: 0.9958\n",
      "Epoch 68/800\n",
      "learning_rate: 0.0001793136\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0190 - accuracy: 0.9976\n",
      "Epoch 69/800\n",
      "learning_rate: 0.00017901261\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0161 - accuracy: 0.9970\n",
      "Epoch 70/800\n",
      "learning_rate: 0.00017871211\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0151 - accuracy: 0.9970\n",
      "Epoch 71/800\n",
      "learning_rate: 0.00017841213\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0151 - accuracy: 0.9964\n",
      "Epoch 72/800\n",
      "learning_rate: 0.00017811265\n",
      "84/84 [==============================] - 44s 527ms/step - loss: 0.0138 - accuracy: 0.9976\n",
      "Epoch 73/800\n",
      "learning_rate: 0.00017781366\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0141 - accuracy: 0.9976\n",
      "Epoch 74/800\n",
      "learning_rate: 0.00017751519\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0188 - accuracy: 0.9946\n",
      "Epoch 75/800\n",
      "learning_rate: 0.00017721721\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0184 - accuracy: 0.9958\n",
      "Epoch 76/800\n",
      "learning_rate: 0.00017691973\n",
      "84/84 [==============================] - 44s 526ms/step - loss: 0.0120 - accuracy: 0.9982\n",
      "Epoch 77/800\n",
      "learning_rate: 0.00017662275\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0144 - accuracy: 0.9970\n",
      "Epoch 78/800\n",
      "learning_rate: 0.00017632627\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0137 - accuracy: 0.9970\n",
      "Epoch 79/800\n",
      "learning_rate: 0.0001760303\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0149 - accuracy: 0.9976\n",
      "Epoch 80/800\n",
      "learning_rate: 0.0001757348\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0132 - accuracy: 0.9976\n",
      "Epoch 81/800\n",
      "learning_rate: 0.00017543981\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0123 - accuracy: 0.9988\n",
      "Epoch 82/800\n",
      "learning_rate: 0.00017514532\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0132 - accuracy: 0.9976\n",
      "Epoch 83/800\n",
      "learning_rate: 0.00017485132\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0134 - accuracy: 0.9976\n",
      "Epoch 84/800\n",
      "learning_rate: 0.00017455782\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0137 - accuracy: 0.9958\n",
      "Epoch 85/800\n",
      "learning_rate: 0.0001742648\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0111 - accuracy: 0.9982\n",
      "Epoch 86/800\n",
      "learning_rate: 0.00017397228\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0156 - accuracy: 0.9970\n",
      "Epoch 87/800\n",
      "learning_rate: 0.00017368025\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0150 - accuracy: 0.9958\n",
      "Epoch 88/800\n",
      "learning_rate: 0.0001733887\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0145 - accuracy: 0.9970\n",
      "Epoch 89/800\n",
      "learning_rate: 0.00017309765\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0180 - accuracy: 0.9940\n",
      "Epoch 90/800\n",
      "learning_rate: 0.0001728071\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0149 - accuracy: 0.9982\n",
      "Epoch 91/800\n",
      "learning_rate: 0.00017251702\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0167 - accuracy: 0.9970\n",
      "Epoch 92/800\n",
      "learning_rate: 0.00017222743\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0144 - accuracy: 0.9988\n",
      "Epoch 93/800\n",
      "learning_rate: 0.00017193833\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0153 - accuracy: 0.9970\n",
      "Epoch 94/800\n",
      "learning_rate: 0.0001716497\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0186 - accuracy: 0.9946\n",
      "Epoch 95/800\n",
      "learning_rate: 0.00017136158\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0099 - accuracy: 0.9976\n",
      "Epoch 96/800\n",
      "learning_rate: 0.00017107393\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0107 - accuracy: 0.9982\n",
      "Epoch 97/800\n",
      "learning_rate: 0.00017078676\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0119 - accuracy: 0.9982\n",
      "Epoch 98/800\n",
      "learning_rate: 0.00017050008\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0119 - accuracy: 0.9988\n",
      "Epoch 99/800\n",
      "learning_rate: 0.00017021388\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0135 - accuracy: 0.9970\n",
      "Epoch 100/800\n",
      "learning_rate: 0.00016992816\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0132 - accuracy: 0.9970\n",
      "Epoch 101/800\n",
      "learning_rate: 0.00016964291\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0129 - accuracy: 0.9988\n",
      "Epoch 102/800\n",
      "learning_rate: 0.00016935814\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 103/800\n",
      "learning_rate: 0.00016907386\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0128 - accuracy: 0.9982\n",
      "Epoch 104/800\n",
      "learning_rate: 0.00016879005\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0127 - accuracy: 0.9970\n",
      "Epoch 105/800\n",
      "learning_rate: 0.00016850673\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0111 - accuracy: 0.9982\n",
      "Epoch 106/800\n",
      "learning_rate: 0.00016822387\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0138 - accuracy: 0.9958\n",
      "Epoch 107/800\n",
      "learning_rate: 0.00016794149\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0113 - accuracy: 0.9982\n",
      "Epoch 108/800\n",
      "learning_rate: 0.00016765957\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0122 - accuracy: 0.9988\n",
      "Epoch 109/800\n",
      "learning_rate: 0.00016737814\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0118 - accuracy: 0.9982\n",
      "Epoch 110/800\n",
      "learning_rate: 0.00016709718\n",
      "84/84 [==============================] - 44s 528ms/step - loss: 0.0126 - accuracy: 0.9982\n",
      "Epoch 111/800\n",
      "learning_rate: 0.0001668167\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0124 - accuracy: 0.9982\n",
      "Epoch 112/800\n",
      "learning_rate: 0.00016653667\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0123 - accuracy: 0.9970\n",
      "Epoch 113/800\n",
      "learning_rate: 0.00016625712\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0118 - accuracy: 0.9982\n",
      "Epoch 114/800\n",
      "learning_rate: 0.00016597804\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0098 - accuracy: 0.9982\n",
      "Epoch 115/800\n",
      "learning_rate: 0.00016569943\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0091 - accuracy: 0.9994\n",
      "Epoch 116/800\n",
      "learning_rate: 0.00016542128\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0146 - accuracy: 0.9970\n",
      "Epoch 117/800\n",
      "learning_rate: 0.0001651436\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0110 - accuracy: 0.9970\n",
      "Epoch 118/800\n",
      "learning_rate: 0.0001648664\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0081 - accuracy: 0.9988\n",
      "Epoch 119/800\n",
      "learning_rate: 0.00016458966\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0109 - accuracy: 0.9982\n",
      "Epoch 120/800\n",
      "learning_rate: 0.00016431337\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0159 - accuracy: 0.9964\n",
      "Epoch 121/800\n",
      "learning_rate: 0.00016403756\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0077 - accuracy: 0.9994\n",
      "Epoch 122/800\n",
      "learning_rate: 0.0001637622\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0074 - accuracy: 0.9982\n",
      "Epoch 123/800\n",
      "learning_rate: 0.0001634873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0097 - accuracy: 0.9982\n",
      "Epoch 124/800\n",
      "learning_rate: 0.00016321287\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 125/800\n",
      "learning_rate: 0.0001629389\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0079 - accuracy: 0.9994\n",
      "Epoch 126/800\n",
      "learning_rate: 0.0001626654\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0092 - accuracy: 0.9982\n",
      "Epoch 127/800\n",
      "learning_rate: 0.00016239235\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0090 - accuracy: 0.9988\n",
      "Epoch 128/800\n",
      "learning_rate: 0.00016211974\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0109 - accuracy: 0.9982\n",
      "Epoch 129/800\n",
      "learning_rate: 0.00016184761\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0098 - accuracy: 0.9994\n",
      "Epoch 130/800\n",
      "learning_rate: 0.00016157594\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0094 - accuracy: 0.9988\n",
      "Epoch 131/800\n",
      "learning_rate: 0.00016130472\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0125 - accuracy: 0.9964\n",
      "Epoch 132/800\n",
      "learning_rate: 0.00016103395\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0092 - accuracy: 0.9982\n",
      "Epoch 133/800\n",
      "learning_rate: 0.00016076364\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0076 - accuracy: 0.9988\n",
      "Epoch 134/800\n",
      "learning_rate: 0.00016049377\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0063 - accuracy: 0.9988\n",
      "Epoch 135/800\n",
      "learning_rate: 0.00016022437\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0101 - accuracy: 0.9982\n",
      "Epoch 136/800\n",
      "learning_rate: 0.00015995542\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0075 - accuracy: 0.9994\n",
      "Epoch 137/800\n",
      "learning_rate: 0.00015968691\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0059 - accuracy: 0.9994\n",
      "Epoch 138/800\n",
      "learning_rate: 0.00015941887\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0117 - accuracy: 0.9982\n",
      "Epoch 139/800\n",
      "learning_rate: 0.00015915126\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0090 - accuracy: 0.9988\n",
      "Epoch 140/800\n",
      "learning_rate: 0.00015888411\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0155 - accuracy: 0.9964\n",
      "Epoch 141/800\n",
      "learning_rate: 0.0001586174\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 142/800\n",
      "learning_rate: 0.00015835115\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0073 - accuracy: 0.9988\n",
      "Epoch 143/800\n",
      "learning_rate: 0.00015808534\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0073 - accuracy: 0.9994\n",
      "Epoch 144/800\n",
      "learning_rate: 0.00015781997\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0080 - accuracy: 0.9970\n",
      "Epoch 145/800\n",
      "learning_rate: 0.00015755506\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0135 - accuracy: 0.9970\n",
      "Epoch 146/800\n",
      "learning_rate: 0.00015729059\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0071 - accuracy: 0.9988\n",
      "Epoch 147/800\n",
      "learning_rate: 0.00015702656\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0068 - accuracy: 0.9994\n",
      "Epoch 148/800\n",
      "learning_rate: 0.00015676297\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0078 - accuracy: 0.9994\n",
      "Epoch 149/800\n",
      "learning_rate: 0.00015649982\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0074 - accuracy: 0.9994\n",
      "Epoch 150/800\n",
      "learning_rate: 0.00015623713\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0066 - accuracy: 0.9988\n",
      "Epoch 151/800\n",
      "learning_rate: 0.00015597486\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0105 - accuracy: 0.9976\n",
      "Epoch 152/800\n",
      "learning_rate: 0.00015571305\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0094 - accuracy: 0.9982\n",
      "Epoch 153/800\n",
      "learning_rate: 0.00015545166\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0082 - accuracy: 0.9988\n",
      "Epoch 154/800\n",
      "learning_rate: 0.00015519072\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0090 - accuracy: 0.9976\n",
      "Epoch 155/800\n",
      "learning_rate: 0.00015493023\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0079 - accuracy: 0.9982\n",
      "Epoch 156/800\n",
      "learning_rate: 0.00015467015\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0081 - accuracy: 0.9970\n",
      "Epoch 157/800\n",
      "learning_rate: 0.00015441052\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0093 - accuracy: 0.9976\n",
      "Epoch 158/800\n",
      "learning_rate: 0.00015415133\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0079 - accuracy: 0.9982\n",
      "Epoch 159/800\n",
      "learning_rate: 0.00015389257\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0068 - accuracy: 0.9988\n",
      "Epoch 160/800\n",
      "learning_rate: 0.00015363425\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0084 - accuracy: 0.9994\n",
      "Epoch 161/800\n",
      "learning_rate: 0.00015337636\n",
      "84/84 [==============================] - 44s 530ms/step - loss: 0.0127 - accuracy: 0.9958\n",
      "Epoch 162/800\n",
      "learning_rate: 0.00015311889\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 163/800\n",
      "learning_rate: 0.00015286186\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0096 - accuracy: 0.9970\n",
      "Epoch 164/800\n",
      "learning_rate: 0.00015260527\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0095 - accuracy: 0.9982\n",
      "Epoch 165/800\n",
      "learning_rate: 0.00015234911\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0061 - accuracy: 0.9994\n",
      "Epoch 166/800\n",
      "learning_rate: 0.00015209337\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0069 - accuracy: 0.9988\n",
      "Epoch 167/800\n",
      "learning_rate: 0.00015183807\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 168/800\n",
      "learning_rate: 0.0001515832\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 169/800\n",
      "learning_rate: 0.00015132874\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 170/800\n",
      "learning_rate: 0.00015107472\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0064 - accuracy: 0.9988\n",
      "Epoch 171/800\n",
      "learning_rate: 0.00015082113\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 172/800\n",
      "learning_rate: 0.00015056795\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0071 - accuracy: 0.9970\n",
      "Epoch 173/800\n",
      "learning_rate: 0.00015031522\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0057 - accuracy: 0.9994\n",
      "Epoch 174/800\n",
      "learning_rate: 0.00015006289\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 175/800\n",
      "learning_rate: 0.00014981099\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0090 - accuracy: 0.9982\n",
      "Epoch 176/800\n",
      "learning_rate: 0.00014955952\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0064 - accuracy: 0.9988\n",
      "Epoch 177/800\n",
      "learning_rate: 0.00014930847\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0082 - accuracy: 0.9988\n",
      "Epoch 178/800\n",
      "learning_rate: 0.00014905784\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 179/800\n",
      "learning_rate: 0.00014880764\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 180/800\n",
      "learning_rate: 0.00014855784\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0069 - accuracy: 0.9994\n",
      "Epoch 181/800\n",
      "learning_rate: 0.00014830848\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0119 - accuracy: 0.9982\n",
      "Epoch 182/800\n",
      "learning_rate: 0.00014805952\n",
      "84/84 [==============================] - 46s 549ms/step - loss: 0.0076 - accuracy: 0.9982\n",
      "Epoch 183/800\n",
      "learning_rate: 0.00014781099\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0082 - accuracy: 0.9994\n",
      "Epoch 184/800\n",
      "learning_rate: 0.00014756287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0059 - accuracy: 0.9994\n",
      "Epoch 185/800\n",
      "learning_rate: 0.00014731516\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0063 - accuracy: 0.9994\n",
      "Epoch 186/800\n",
      "learning_rate: 0.00014706788\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 187/800\n",
      "learning_rate: 0.00014682101\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0072 - accuracy: 0.9994\n",
      "Epoch 188/800\n",
      "learning_rate: 0.00014657456\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 189/800\n",
      "learning_rate: 0.00014632852\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 190/800\n",
      "learning_rate: 0.0001460829\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 191/800\n",
      "learning_rate: 0.00014583768\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 192/800\n",
      "learning_rate: 0.00014559287\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 193/800\n",
      "learning_rate: 0.00014534849\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0081 - accuracy: 0.9988\n",
      "Epoch 194/800\n",
      "learning_rate: 0.0001451045\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0073 - accuracy: 0.9982\n",
      "Epoch 195/800\n",
      "learning_rate: 0.00014486093\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0057 - accuracy: 0.9994\n",
      "Epoch 196/800\n",
      "learning_rate: 0.00014461776\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 197/800\n",
      "learning_rate: 0.00014437501\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 198/800\n",
      "learning_rate: 0.00014413266\n",
      "84/84 [==============================] - 45s 536ms/step - loss: 0.0049 - accuracy: 0.9994\n",
      "Epoch 199/800\n",
      "learning_rate: 0.00014389072\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0074 - accuracy: 0.9988\n",
      "Epoch 200/800\n",
      "learning_rate: 0.00014364917\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 201/800\n",
      "learning_rate: 0.00014340805\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0056 - accuracy: 0.9994\n",
      "Epoch 202/800\n",
      "learning_rate: 0.00014316732\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0068 - accuracy: 0.9988\n",
      "Epoch 203/800\n",
      "learning_rate: 0.000142927\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0073 - accuracy: 0.9988\n",
      "Epoch 204/800\n",
      "learning_rate: 0.00014268709\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0120 - accuracy: 0.9982\n",
      "Epoch 205/800\n",
      "learning_rate: 0.00014244756\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 206/800\n",
      "learning_rate: 0.00014220845\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0056 - accuracy: 0.9994\n",
      "Epoch 207/800\n",
      "learning_rate: 0.00014196974\n",
      "84/84 [==============================] - 45s 536ms/step - loss: 0.0073 - accuracy: 0.9994\n",
      "Epoch 208/800\n",
      "learning_rate: 0.00014173143\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0053 - accuracy: 0.9982\n",
      "Epoch 209/800\n",
      "learning_rate: 0.00014149353\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 210/800\n",
      "learning_rate: 0.00014125601\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 211/800\n",
      "learning_rate: 0.0001410189\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0073 - accuracy: 0.9994\n",
      "Epoch 212/800\n",
      "learning_rate: 0.00014078218\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 213/800\n",
      "learning_rate: 0.00014054586\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 214/800\n",
      "learning_rate: 0.00014030994\n",
      "84/84 [==============================] - 45s 536ms/step - loss: 0.0063 - accuracy: 0.9994\n",
      "Epoch 215/800\n",
      "learning_rate: 0.00014007441\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0075 - accuracy: 0.9988\n",
      "Epoch 216/800\n",
      "learning_rate: 0.00013983928\n",
      "84/84 [==============================] - 45s 539ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 217/800\n",
      "learning_rate: 0.00013960456\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 218/800\n",
      "learning_rate: 0.00013937021\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 219/800\n",
      "learning_rate: 0.00013913626\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0075 - accuracy: 0.9982\n",
      "Epoch 220/800\n",
      "learning_rate: 0.0001389027\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 221/800\n",
      "learning_rate: 0.00013866954\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0056 - accuracy: 0.9994\n",
      "Epoch 222/800\n",
      "learning_rate: 0.00013843676\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 223/800\n",
      "learning_rate: 0.00013820438\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 224/800\n",
      "learning_rate: 0.0001379724\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0085 - accuracy: 0.9976\n",
      "Epoch 225/800\n",
      "learning_rate: 0.0001377408\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 226/800\n",
      "learning_rate: 0.00013750959\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0077 - accuracy: 0.9982\n",
      "Epoch 227/800\n",
      "learning_rate: 0.00013727877\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0065 - accuracy: 0.9994\n",
      "Epoch 228/800\n",
      "learning_rate: 0.00013704832\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0056 - accuracy: 0.9982\n",
      "Epoch 229/800\n",
      "learning_rate: 0.00013681827\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0074 - accuracy: 0.9988\n",
      "Epoch 230/800\n",
      "learning_rate: 0.00013658861\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0058 - accuracy: 0.9994\n",
      "Epoch 231/800\n",
      "learning_rate: 0.00013635933\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 232/800\n",
      "learning_rate: 0.00013613043\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 233/800\n",
      "learning_rate: 0.00013590192\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0059 - accuracy: 0.9994\n",
      "Epoch 234/800\n",
      "learning_rate: 0.0001356738\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 235/800\n",
      "learning_rate: 0.00013544605\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 236/800\n",
      "learning_rate: 0.0001352187\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 237/800\n",
      "learning_rate: 0.00013499171\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0095 - accuracy: 0.9982\n",
      "Epoch 238/800\n",
      "learning_rate: 0.00013476513\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 239/800\n",
      "learning_rate: 0.0001345389\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 240/800\n",
      "learning_rate: 0.00013431307\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 241/800\n",
      "learning_rate: 0.0001340876\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 242/800\n",
      "learning_rate: 0.00013386253\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0057 - accuracy: 0.9988\n",
      "Epoch 243/800\n",
      "learning_rate: 0.00013363782\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 244/800\n",
      "learning_rate: 0.0001334135\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 245/800\n",
      "learning_rate: 0.00013318955\n",
      "84/84 [==============================] - 45s 538ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 246/800\n",
      "learning_rate: 0.00013296597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 247/800\n",
      "learning_rate: 0.00013274277\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 248/800\n",
      "learning_rate: 0.00013251996\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 249/800\n",
      "learning_rate: 0.00013229751\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 250/800\n",
      "learning_rate: 0.00013207544\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0061 - accuracy: 0.9982\n",
      "Epoch 251/800\n",
      "learning_rate: 0.00013185373\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0055 - accuracy: 0.9994\n",
      "Epoch 252/800\n",
      "learning_rate: 0.0001316324\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 253/800\n",
      "learning_rate: 0.00013141143\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0060 - accuracy: 0.9982\n",
      "Epoch 254/800\n",
      "learning_rate: 0.00013119086\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 255/800\n",
      "learning_rate: 0.00013097063\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 256/800\n",
      "learning_rate: 0.00013075079\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0066 - accuracy: 0.9988\n",
      "Epoch 257/800\n",
      "learning_rate: 0.0001305313\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 258/800\n",
      "learning_rate: 0.0001303122\n",
      "84/84 [==============================] - 45s 537ms/step - loss: 0.0067 - accuracy: 0.9988\n",
      "Epoch 259/800\n",
      "learning_rate: 0.00013009345\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0065 - accuracy: 0.9976\n",
      "Epoch 260/800\n",
      "learning_rate: 0.00012987507\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 261/800\n",
      "learning_rate: 0.00012965707\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 262/800\n",
      "learning_rate: 0.00012943943\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 263/800\n",
      "learning_rate: 0.00012922214\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0064 - accuracy: 0.9994\n",
      "Epoch 264/800\n",
      "learning_rate: 0.00012900523\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 265/800\n",
      "learning_rate: 0.00012878868\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0068 - accuracy: 0.9982\n",
      "Epoch 266/800\n",
      "learning_rate: 0.0001285725\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 267/800\n",
      "learning_rate: 0.00012835668\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0061 - accuracy: 0.9988\n",
      "Epoch 268/800\n",
      "learning_rate: 0.00012814121\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0064 - accuracy: 0.9988\n",
      "Epoch 269/800\n",
      "learning_rate: 0.00012792612\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 270/800\n",
      "learning_rate: 0.00012771138\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0089 - accuracy: 0.9982\n",
      "Epoch 271/800\n",
      "learning_rate: 0.000127497\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 272/800\n",
      "learning_rate: 0.00012728298\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0049 - accuracy: 0.9994\n",
      "Epoch 273/800\n",
      "learning_rate: 0.00012706933\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 274/800\n",
      "learning_rate: 0.00012685603\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0064 - accuracy: 0.9994\n",
      "Epoch 275/800\n",
      "learning_rate: 0.00012664309\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0059 - accuracy: 0.9988\n",
      "Epoch 276/800\n",
      "learning_rate: 0.0001264305\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 277/800\n",
      "learning_rate: 0.00012621828\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0076 - accuracy: 0.9988\n",
      "Epoch 278/800\n",
      "learning_rate: 0.0001260064\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0068 - accuracy: 0.9988\n",
      "Epoch 279/800\n",
      "learning_rate: 0.00012579489\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 280/800\n",
      "learning_rate: 0.00012558373\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0066 - accuracy: 0.9988\n",
      "Epoch 281/800\n",
      "learning_rate: 0.00012537293\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 282/800\n",
      "learning_rate: 0.00012516248\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 283/800\n",
      "learning_rate: 0.00012495238\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0057 - accuracy: 0.9988\n",
      "Epoch 284/800\n",
      "learning_rate: 0.00012474263\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 285/800\n",
      "learning_rate: 0.00012453324\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 286/800\n",
      "learning_rate: 0.00012432419\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9994\n",
      "Epoch 287/800\n",
      "learning_rate: 0.0001241155\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 288/800\n",
      "learning_rate: 0.00012390716\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 289/800\n",
      "learning_rate: 0.00012369917\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0072 - accuracy: 0.9970\n",
      "Epoch 290/800\n",
      "learning_rate: 0.00012349153\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 291/800\n",
      "learning_rate: 0.00012328423\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 292/800\n",
      "learning_rate: 0.00012307729\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 293/800\n",
      "learning_rate: 0.00012287068\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 294/800\n",
      "learning_rate: 0.00012266444\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 295/800\n",
      "learning_rate: 0.00012245853\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 296/800\n",
      "learning_rate: 0.00012225297\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 297/800\n",
      "learning_rate: 0.00012204776\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 298/800\n",
      "learning_rate: 0.00012184289\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 299/800\n",
      "learning_rate: 0.00012163836\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 300/800\n",
      "learning_rate: 0.000121434176\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 301/800\n",
      "learning_rate: 0.00012123034\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 302/800\n",
      "learning_rate: 0.00012102684\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 303/800\n",
      "learning_rate: 0.000120823686\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 304/800\n",
      "learning_rate: 0.00012062087\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 305/800\n",
      "learning_rate: 0.00012041839\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 306/800\n",
      "learning_rate: 0.00012021626\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 307/800\n",
      "learning_rate: 0.00012001446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 308/800\n",
      "learning_rate: 0.000119813005\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 309/800\n",
      "learning_rate: 0.00011961189\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0069 - accuracy: 0.9982\n",
      "Epoch 310/800\n",
      "learning_rate: 0.0001194111\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 311/800\n",
      "learning_rate: 0.000119210665\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0087 - accuracy: 0.9988\n",
      "Epoch 312/800\n",
      "learning_rate: 0.000119010554\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0067 - accuracy: 0.9976\n",
      "Epoch 313/800\n",
      "learning_rate: 0.000118810785\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 314/800\n",
      "learning_rate: 0.000118611344\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 315/800\n",
      "learning_rate: 0.000118412245\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 316/800\n",
      "learning_rate: 0.00011821347\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 317/800\n",
      "learning_rate: 0.00011801504\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9982\n",
      "Epoch 318/800\n",
      "learning_rate: 0.00011781694\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 319/800\n",
      "learning_rate: 0.00011761917\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 320/800\n",
      "learning_rate: 0.00011742174\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0068 - accuracy: 0.9988\n",
      "Epoch 321/800\n",
      "learning_rate: 0.000117224634\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 322/800\n",
      "learning_rate: 0.000117027856\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 323/800\n",
      "learning_rate: 0.00011683141\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 324/800\n",
      "learning_rate: 0.0001166353\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 325/800\n",
      "learning_rate: 0.000116439514\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Epoch 326/800\n",
      "learning_rate: 0.00011624406\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 327/800\n",
      "learning_rate: 0.000116048934\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0063 - accuracy: 0.9976\n",
      "Epoch 328/800\n",
      "learning_rate: 0.00011585413\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 329/800\n",
      "learning_rate: 0.000115659655\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 330/800\n",
      "learning_rate: 0.00011546551\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 331/800\n",
      "learning_rate: 0.00011527169\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 332/800\n",
      "learning_rate: 0.00011507819\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0058 - accuracy: 0.9982\n",
      "Epoch 333/800\n",
      "learning_rate: 0.00011488502\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 334/800\n",
      "learning_rate: 0.00011469218\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 0.9982\n",
      "Epoch 335/800\n",
      "learning_rate: 0.00011449965\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 336/800\n",
      "learning_rate: 0.000114307455\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 337/800\n",
      "learning_rate: 0.00011411557\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 338/800\n",
      "learning_rate: 0.00011392402\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 339/800\n",
      "learning_rate: 0.000113732785\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 340/800\n",
      "learning_rate: 0.00011354187\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 341/800\n",
      "learning_rate: 0.000113351285\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0055 - accuracy: 0.9994\n",
      "Epoch 342/800\n",
      "learning_rate: 0.00011316101\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 343/800\n",
      "learning_rate: 0.00011297106\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 344/800\n",
      "learning_rate: 0.000112781425\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 345/800\n",
      "learning_rate: 0.000112592104\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 346/800\n",
      "learning_rate: 0.00011240311\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 347/800\n",
      "learning_rate: 0.00011221443\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 348/800\n",
      "learning_rate: 0.000112026064\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 0.9988\n",
      "Epoch 349/800\n",
      "learning_rate: 0.00011183802\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 350/800\n",
      "learning_rate: 0.00011165028\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 351/800\n",
      "learning_rate: 0.00011146287\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 352/800\n",
      "learning_rate: 0.00011127577\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 353/800\n",
      "learning_rate: 0.00011108898\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 354/800\n",
      "learning_rate: 0.0001109025\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 355/800\n",
      "learning_rate: 0.00011071634\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 356/800\n",
      "learning_rate: 0.00011053049\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0056 - accuracy: 0.9982\n",
      "Epoch 357/800\n",
      "learning_rate: 0.000110344954\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 358/800\n",
      "learning_rate: 0.00011015973\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 359/800\n",
      "learning_rate: 0.00010997481\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 360/800\n",
      "learning_rate: 0.00010979021\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 361/800\n",
      "learning_rate: 0.00010960591\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 362/800\n",
      "learning_rate: 0.00010942193\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 363/800\n",
      "learning_rate: 0.00010923825\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 364/800\n",
      "learning_rate: 0.00010905488\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 365/800\n",
      "learning_rate: 0.00010887183\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 366/800\n",
      "learning_rate: 0.00010868908\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0069 - accuracy: 0.9982\n",
      "Epoch 367/800\n",
      "learning_rate: 0.000108506625\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 368/800\n",
      "learning_rate: 0.000108324486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 369/800\n",
      "learning_rate: 0.00010814265\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 370/800\n",
      "learning_rate: 0.000107961125\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 371/800\n",
      "learning_rate: 0.0001077799\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 372/800\n",
      "learning_rate: 0.00010759898\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 373/800\n",
      "learning_rate: 0.00010741836\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 0.9994\n",
      "Epoch 374/800\n",
      "learning_rate: 0.00010723805\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 375/800\n",
      "learning_rate: 0.00010705804\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 376/800\n",
      "learning_rate: 0.00010687833\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 377/800\n",
      "learning_rate: 0.00010669893\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 378/800\n",
      "learning_rate: 0.00010651982\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 379/800\n",
      "learning_rate: 0.00010634102\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 380/800\n",
      "learning_rate: 0.00010616251\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 381/800\n",
      "learning_rate: 0.000105984305\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 382/800\n",
      "learning_rate: 0.0001058064\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 383/800\n",
      "learning_rate: 0.000105628795\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 384/800\n",
      "learning_rate: 0.00010545148\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 385/800\n",
      "learning_rate: 0.00010527447\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 386/800\n",
      "learning_rate: 0.00010509776\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0034 - accuracy: 0.9988\n",
      "Epoch 387/800\n",
      "learning_rate: 0.00010492134\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 388/800\n",
      "learning_rate: 0.00010474522\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 389/800\n",
      "learning_rate: 0.00010456939\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 390/800\n",
      "learning_rate: 0.00010439386\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 391/800\n",
      "learning_rate: 0.00010421863\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0052 - accuracy: 0.9988\n",
      "Epoch 392/800\n",
      "learning_rate: 0.000104043684\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 393/800\n",
      "learning_rate: 0.00010386903\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 394/800\n",
      "learning_rate: 0.00010369468\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 395/800\n",
      "learning_rate: 0.000103520615\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 396/800\n",
      "learning_rate: 0.00010334684\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 397/800\n",
      "learning_rate: 0.00010317337\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 398/800\n",
      "learning_rate: 0.00010300018\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 399/800\n",
      "learning_rate: 0.00010282728\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 400/800\n",
      "learning_rate: 0.000102654674\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 401/800\n",
      "learning_rate: 0.00010248236\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 402/800\n",
      "learning_rate: 0.00010231033\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0049 - accuracy: 0.9994\n",
      "Epoch 403/800\n",
      "learning_rate: 0.00010213859\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 404/800\n",
      "learning_rate: 0.00010196714\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 405/800\n",
      "learning_rate: 0.00010179598\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 406/800\n",
      "learning_rate: 0.000101625104\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0090 - accuracy: 0.9976\n",
      "Epoch 407/800\n",
      "learning_rate: 0.00010145451\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 408/800\n",
      "learning_rate: 0.00010128421\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 409/800\n",
      "learning_rate: 0.000101114194\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 410/800\n",
      "learning_rate: 0.00010094447\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 411/800\n",
      "learning_rate: 0.00010077502\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 412/800\n",
      "learning_rate: 0.00010060586\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 413/800\n",
      "learning_rate: 0.000100436984\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 414/800\n",
      "learning_rate: 0.000100268386\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 415/800\n",
      "learning_rate: 0.00010010008\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0048 - accuracy: 0.9988\n",
      "Epoch 416/800\n",
      "learning_rate: 9.993205e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 417/800\n",
      "learning_rate: 9.97643e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 418/800\n",
      "learning_rate: 9.959684e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 419/800\n",
      "learning_rate: 9.942965e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0057 - accuracy: 0.9994\n",
      "Epoch 420/800\n",
      "learning_rate: 9.926275e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 421/800\n",
      "learning_rate: 9.909613e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 422/800\n",
      "learning_rate: 9.8929784e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 423/800\n",
      "learning_rate: 9.876372e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 424/800\n",
      "learning_rate: 9.8597935e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0048 - accuracy: 0.9988\n",
      "Epoch 425/800\n",
      "learning_rate: 9.843243e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 426/800\n",
      "learning_rate: 9.82672e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0070 - accuracy: 0.9988\n",
      "Epoch 427/800\n",
      "learning_rate: 9.8102246e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 428/800\n",
      "learning_rate: 9.793757e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 429/800\n",
      "learning_rate: 9.777317e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 430/800\n",
      "learning_rate: 9.7609045e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 0.9982\n",
      "Epoch 431/800\n",
      "learning_rate: 9.74452e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 432/800\n",
      "learning_rate: 9.728163e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 433/800\n",
      "learning_rate: 9.711833e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 434/800\n",
      "learning_rate: 9.695531e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 435/800\n",
      "learning_rate: 9.679256e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0076 - accuracy: 0.9982\n",
      "Epoch 436/800\n",
      "learning_rate: 9.663008e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 437/800\n",
      "learning_rate: 9.646788e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 438/800\n",
      "learning_rate: 9.630594e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 439/800\n",
      "learning_rate: 9.6144286e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 440/800\n",
      "learning_rate: 9.59829e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 441/800\n",
      "learning_rate: 9.582178e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 442/800\n",
      "learning_rate: 9.566094e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 443/800\n",
      "learning_rate: 9.5500356e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9982\n",
      "Epoch 444/800\n",
      "learning_rate: 9.5340045e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 445/800\n",
      "learning_rate: 9.518001e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 446/800\n",
      "learning_rate: 9.502024e-05\n",
      "84/84 [==============================] - 45s 535ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 447/800\n",
      "learning_rate: 9.486074e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0061 - accuracy: 0.9988\n",
      "Epoch 448/800\n",
      "learning_rate: 9.470151e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 449/800\n",
      "learning_rate: 9.454254e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 450/800\n",
      "learning_rate: 9.438384e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 451/800\n",
      "learning_rate: 9.4225405e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 452/800\n",
      "learning_rate: 9.406724e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 453/800\n",
      "learning_rate: 9.390934e-05\n",
      "84/84 [==============================] - 46s 543ms/step - loss: 0.0058 - accuracy: 0.9976\n",
      "Epoch 454/800\n",
      "learning_rate: 9.3751696e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 455/800\n",
      "learning_rate: 9.3594324e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 456/800\n",
      "learning_rate: 9.343722e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 457/800\n",
      "learning_rate: 9.3280374e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 458/800\n",
      "learning_rate: 9.3123796e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 459/800\n",
      "learning_rate: 9.296748e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 460/800\n",
      "learning_rate: 9.281142e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 461/800\n",
      "learning_rate: 9.2655624e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 462/800\n",
      "learning_rate: 9.250009e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 463/800\n",
      "learning_rate: 9.2344824e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 464/800\n",
      "learning_rate: 9.218981e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 465/800\n",
      "learning_rate: 9.2035065e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 466/800\n",
      "learning_rate: 9.188057e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0086 - accuracy: 0.9976\n",
      "Epoch 467/800\n",
      "learning_rate: 9.172634e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 468/800\n",
      "learning_rate: 9.1572365e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 469/800\n",
      "learning_rate: 9.141865e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 470/800\n",
      "learning_rate: 9.1265196e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 471/800\n",
      "learning_rate: 9.1112e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 472/800\n",
      "learning_rate: 9.095906e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0056 - accuracy: 0.9982\n",
      "Epoch 473/800\n",
      "learning_rate: 9.0806374e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 474/800\n",
      "learning_rate: 9.065394e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 0.9994\n",
      "Epoch 475/800\n",
      "learning_rate: 9.050177e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 476/800\n",
      "learning_rate: 9.034986e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 477/800\n",
      "learning_rate: 9.019819e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 478/800\n",
      "learning_rate: 9.0046786e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 479/800\n",
      "learning_rate: 8.9895635e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 480/800\n",
      "learning_rate: 8.974473e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0086 - accuracy: 0.9970\n",
      "Epoch 481/800\n",
      "learning_rate: 8.959409e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 482/800\n",
      "learning_rate: 8.9443696e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 483/800\n",
      "learning_rate: 8.929356e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 484/800\n",
      "learning_rate: 8.9143665e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 485/800\n",
      "learning_rate: 8.899403e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 486/800\n",
      "learning_rate: 8.8844645e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 487/800\n",
      "learning_rate: 8.869551e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 488/800\n",
      "learning_rate: 8.854662e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 489/800\n",
      "learning_rate: 8.839799e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 490/800\n",
      "learning_rate: 8.82496e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 491/800\n",
      "learning_rate: 8.8101464e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 492/800\n",
      "learning_rate: 8.795358e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0043 - accuracy: 0.9988\n",
      "Epoch 493/800\n",
      "learning_rate: 8.7805936e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 494/800\n",
      "learning_rate: 8.765855e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 495/800\n",
      "learning_rate: 8.7511406e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 496/800\n",
      "learning_rate: 8.7364504e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 497/800\n",
      "learning_rate: 8.721786e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 498/800\n",
      "learning_rate: 8.707145e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0059 - accuracy: 0.9988\n",
      "Epoch 499/800\n",
      "learning_rate: 8.692529e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 500/800\n",
      "learning_rate: 8.677938e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 501/800\n",
      "learning_rate: 8.663371e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 502/800\n",
      "learning_rate: 8.648829e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 503/800\n",
      "learning_rate: 8.6343105e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 504/800\n",
      "learning_rate: 8.619817e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0062 - accuracy: 0.9964\n",
      "Epoch 505/800\n",
      "learning_rate: 8.605348e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0051 - accuracy: 0.9994\n",
      "Epoch 506/800\n",
      "learning_rate: 8.590903e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 507/800\n",
      "learning_rate: 8.576482e-05\n",
      "84/84 [==============================] - 44s 529ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 508/800\n",
      "learning_rate: 8.562086e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 509/800\n",
      "learning_rate: 8.547713e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 510/800\n",
      "learning_rate: 8.5333646e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 511/800\n",
      "learning_rate: 8.5190404e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 512/800\n",
      "learning_rate: 8.50474e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 513/800\n",
      "learning_rate: 8.490464e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 514/800\n",
      "learning_rate: 8.476212e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 515/800\n",
      "learning_rate: 8.461984e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 516/800\n",
      "learning_rate: 8.44778e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9982\n",
      "Epoch 517/800\n",
      "learning_rate: 8.4335996e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 518/800\n",
      "learning_rate: 8.419443e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 519/800\n",
      "learning_rate: 8.40531e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 520/800\n",
      "learning_rate: 8.3912004e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 521/800\n",
      "learning_rate: 8.377115e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0064 - accuracy: 0.9994\n",
      "Epoch 522/800\n",
      "learning_rate: 8.363053e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 523/800\n",
      "learning_rate: 8.3490144e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0059 - accuracy: 0.9988\n",
      "Epoch 524/800\n",
      "learning_rate: 8.335e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 525/800\n",
      "learning_rate: 8.3210085e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 526/800\n",
      "learning_rate: 8.307041e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0066 - accuracy: 0.9982\n",
      "Epoch 527/800\n",
      "learning_rate: 8.293097e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 528/800\n",
      "learning_rate: 8.279176e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 529/800\n",
      "learning_rate: 8.265278e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 530/800\n",
      "learning_rate: 8.2514045e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 531/800\n",
      "learning_rate: 8.237553e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 532/800\n",
      "learning_rate: 8.223726e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 533/800\n",
      "learning_rate: 8.209921e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 534/800\n",
      "learning_rate: 8.196141e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 535/800\n",
      "learning_rate: 8.1823826e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0070 - accuracy: 0.9988\n",
      "Epoch 536/800\n",
      "learning_rate: 8.168647e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 537/800\n",
      "learning_rate: 8.1549355e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 538/800\n",
      "learning_rate: 8.1412465e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 539/800\n",
      "learning_rate: 8.12758e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 540/800\n",
      "learning_rate: 8.113938e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 541/800\n",
      "learning_rate: 8.100317e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 0.9982\n",
      "Epoch 542/800\n",
      "learning_rate: 8.08672e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 543/800\n",
      "learning_rate: 8.073146e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0075 - accuracy: 0.9988\n",
      "Epoch 544/800\n",
      "learning_rate: 8.059594e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 545/800\n",
      "learning_rate: 8.046065e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 546/800\n",
      "learning_rate: 8.032559e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 547/800\n",
      "learning_rate: 8.019075e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 548/800\n",
      "learning_rate: 8.005615e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 549/800\n",
      "learning_rate: 7.992176e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0056 - accuracy: 0.9994\n",
      "Epoch 550/800\n",
      "learning_rate: 7.9787606e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 551/800\n",
      "learning_rate: 7.965368e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 552/800\n",
      "learning_rate: 7.951997e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 553/800\n",
      "learning_rate: 7.938648e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0054 - accuracy: 0.9994\n",
      "Epoch 554/800\n",
      "learning_rate: 7.925322e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0057 - accuracy: 0.9982\n",
      "Epoch 555/800\n",
      "learning_rate: 7.912019e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0083 - accuracy: 0.9982\n",
      "Epoch 556/800\n",
      "learning_rate: 7.898738e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 557/800\n",
      "learning_rate: 7.885479e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 558/800\n",
      "learning_rate: 7.872243e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 559/800\n",
      "learning_rate: 7.859028e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 560/800\n",
      "learning_rate: 7.845836e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 561/800\n",
      "learning_rate: 7.832666e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 562/800\n",
      "learning_rate: 7.8195175e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 563/800\n",
      "learning_rate: 7.806392e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 564/800\n",
      "learning_rate: 7.793288e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 565/800\n",
      "learning_rate: 7.780206e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0048 - accuracy: 0.9988\n",
      "Epoch 566/800\n",
      "learning_rate: 7.767146e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 567/800\n",
      "learning_rate: 7.754108e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 568/800\n",
      "learning_rate: 7.741092e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 569/800\n",
      "learning_rate: 7.728098e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 570/800\n",
      "learning_rate: 7.715126e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 571/800\n",
      "learning_rate: 7.7021745e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 572/800\n",
      "learning_rate: 7.689246e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 573/800\n",
      "learning_rate: 7.676339e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 574/800\n",
      "learning_rate: 7.663453e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 575/800\n",
      "learning_rate: 7.6505894e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0073 - accuracy: 0.9988\n",
      "Epoch 576/800\n",
      "learning_rate: 7.6377466e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 577/800\n",
      "learning_rate: 7.6249264e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 578/800\n",
      "learning_rate: 7.6121265e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 579/800\n",
      "learning_rate: 7.599349e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 580/800\n",
      "learning_rate: 7.586593e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 581/800\n",
      "learning_rate: 7.573858e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 582/800\n",
      "learning_rate: 7.5611446e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 583/800\n",
      "learning_rate: 7.5484524e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 584/800\n",
      "learning_rate: 7.535781e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 585/800\n",
      "learning_rate: 7.523131e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 586/800\n",
      "learning_rate: 7.510503e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 587/800\n",
      "learning_rate: 7.497896e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 588/800\n",
      "learning_rate: 7.48531e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 589/800\n",
      "learning_rate: 7.472745e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 590/800\n",
      "learning_rate: 7.4602016e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 591/800\n",
      "learning_rate: 7.447679e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 592/800\n",
      "learning_rate: 7.435177e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 593/800\n",
      "learning_rate: 7.422696e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 594/800\n",
      "learning_rate: 7.410236e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 595/800\n",
      "learning_rate: 7.397798e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 596/800\n",
      "learning_rate: 7.385379e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 597/800\n",
      "learning_rate: 7.3729825e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 598/800\n",
      "learning_rate: 7.360606e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 599/800\n",
      "learning_rate: 7.348251e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 600/800\n",
      "learning_rate: 7.335916e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 601/800\n",
      "learning_rate: 7.323601e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 602/800\n",
      "learning_rate: 7.311308e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 603/800\n",
      "learning_rate: 7.2990355e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 604/800\n",
      "learning_rate: 7.286783e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 605/800\n",
      "learning_rate: 7.274551e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 606/800\n",
      "learning_rate: 7.262341e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 607/800\n",
      "learning_rate: 7.25015e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 608/800\n",
      "learning_rate: 7.237979e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 609/800\n",
      "learning_rate: 7.22583e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 610/800\n",
      "learning_rate: 7.213701e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 611/800\n",
      "learning_rate: 7.2015915e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0064 - accuracy: 0.9982\n",
      "Epoch 612/800\n",
      "learning_rate: 7.189503e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 613/800\n",
      "learning_rate: 7.1774346e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 614/800\n",
      "learning_rate: 7.165386e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 615/800\n",
      "learning_rate: 7.1533585e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 616/800\n",
      "learning_rate: 7.141351e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 617/800\n",
      "learning_rate: 7.129364e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0060 - accuracy: 0.9994\n",
      "Epoch 618/800\n",
      "learning_rate: 7.117396e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 619/800\n",
      "learning_rate: 7.1054485e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0095 - accuracy: 0.9976\n",
      "Epoch 620/800\n",
      "learning_rate: 7.093522e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0045 - accuracy: 0.9982\n",
      "Epoch 621/800\n",
      "learning_rate: 7.0816146e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 622/800\n",
      "learning_rate: 7.069727e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0049 - accuracy: 0.9982\n",
      "Epoch 623/800\n",
      "learning_rate: 7.05786e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 624/800\n",
      "learning_rate: 7.0460126e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 625/800\n",
      "learning_rate: 7.034185e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 626/800\n",
      "learning_rate: 7.0223774e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 627/800\n",
      "learning_rate: 7.0105896e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 628/800\n",
      "learning_rate: 6.9988215e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 629/800\n",
      "learning_rate: 6.987073e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 630/800\n",
      "learning_rate: 6.975345e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 631/800\n",
      "learning_rate: 6.963636e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 632/800\n",
      "learning_rate: 6.951947e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 633/800\n",
      "learning_rate: 6.940277e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 634/800\n",
      "learning_rate: 6.928627e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 635/800\n",
      "learning_rate: 6.916997e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0043 - accuracy: 0.9988\n",
      "Epoch 636/800\n",
      "learning_rate: 6.905386e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 637/800\n",
      "learning_rate: 6.8937945e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 638/800\n",
      "learning_rate: 6.882222e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 639/800\n",
      "learning_rate: 6.87067e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 640/800\n",
      "learning_rate: 6.859137e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 641/800\n",
      "learning_rate: 6.847623e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 642/800\n",
      "learning_rate: 6.836128e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 643/800\n",
      "learning_rate: 6.824653e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 644/800\n",
      "learning_rate: 6.813197e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 645/800\n",
      "learning_rate: 6.801761e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 646/800\n",
      "learning_rate: 6.7903435e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0061 - accuracy: 0.9982\n",
      "Epoch 647/800\n",
      "learning_rate: 6.778945e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 648/800\n",
      "learning_rate: 6.767565e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 649/800\n",
      "learning_rate: 6.7562054e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 650/800\n",
      "learning_rate: 6.7448644e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 651/800\n",
      "learning_rate: 6.733542e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 652/800\n",
      "learning_rate: 6.72224e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 653/800\n",
      "learning_rate: 6.7109555e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 654/800\n",
      "learning_rate: 6.699691e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 655/800\n",
      "learning_rate: 6.6884444e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 656/800\n",
      "learning_rate: 6.677217e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 657/800\n",
      "learning_rate: 6.666009e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 658/800\n",
      "learning_rate: 6.654819e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 659/800\n",
      "learning_rate: 6.6436485e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 660/800\n",
      "learning_rate: 6.632496e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 661/800\n",
      "learning_rate: 6.621363e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 662/800\n",
      "learning_rate: 6.610248e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 663/800\n",
      "learning_rate: 6.5991524e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 664/800\n",
      "learning_rate: 6.588075e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 665/800\n",
      "learning_rate: 6.577016e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 666/800\n",
      "learning_rate: 6.5659755e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 667/800\n",
      "learning_rate: 6.554954e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 668/800\n",
      "learning_rate: 6.543951e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 669/800\n",
      "learning_rate: 6.532966e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 670/800\n",
      "learning_rate: 6.5219996e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 671/800\n",
      "learning_rate: 6.511052e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 672/800\n",
      "learning_rate: 6.500122e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0060 - accuracy: 0.9982\n",
      "Epoch 673/800\n",
      "learning_rate: 6.489211e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0064 - accuracy: 0.9976\n",
      "Epoch 674/800\n",
      "learning_rate: 6.4783184e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0043 - accuracy: 0.9988\n",
      "Epoch 675/800\n",
      "learning_rate: 6.467444e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 676/800\n",
      "learning_rate: 6.456588e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 677/800\n",
      "learning_rate: 6.44575e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 678/800\n",
      "learning_rate: 6.43493e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0066 - accuracy: 0.9988\n",
      "Epoch 679/800\n",
      "learning_rate: 6.424128e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 680/800\n",
      "learning_rate: 6.413345e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 681/800\n",
      "learning_rate: 6.402579e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 682/800\n",
      "learning_rate: 6.391831e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 683/800\n",
      "learning_rate: 6.381102e-05\n",
      "84/84 [==============================] - 46s 542ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 684/800\n",
      "learning_rate: 6.370391e-05\n",
      "84/84 [==============================] - 46s 542ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 685/800\n",
      "learning_rate: 6.359698e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 686/800\n",
      "learning_rate: 6.3490224e-05\n",
      "84/84 [==============================] - 46s 542ms/step - loss: 0.0030 - accuracy: 0.9988\n",
      "Epoch 687/800\n",
      "learning_rate: 6.3383646e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 688/800\n",
      "learning_rate: 6.327725e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 689/800\n",
      "learning_rate: 6.3171035e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 690/800\n",
      "learning_rate: 6.3064996e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 691/800\n",
      "learning_rate: 6.295913e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 692/800\n",
      "learning_rate: 6.285345e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 693/800\n",
      "learning_rate: 6.274794e-05\n",
      "84/84 [==============================] - 45s 541ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 694/800\n",
      "learning_rate: 6.264261e-05\n",
      "84/84 [==============================] - 45s 540ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 695/800\n",
      "learning_rate: 6.253746e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 696/800\n",
      "learning_rate: 6.243248e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 697/800\n",
      "learning_rate: 6.232769e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 698/800\n",
      "learning_rate: 6.222306e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 699/800\n",
      "learning_rate: 6.211861e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 700/800\n",
      "learning_rate: 6.201434e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 701/800\n",
      "learning_rate: 6.191024e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 702/800\n",
      "learning_rate: 6.180632e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 703/800\n",
      "learning_rate: 6.170257e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 704/800\n",
      "learning_rate: 6.1599e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 705/800\n",
      "learning_rate: 6.149559e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 706/800\n",
      "learning_rate: 6.139237e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 707/800\n",
      "learning_rate: 6.128931e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 708/800\n",
      "learning_rate: 6.118644e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0046 - accuracy: 0.9988\n",
      "Epoch 709/800\n",
      "learning_rate: 6.108373e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 710/800\n",
      "learning_rate: 6.098119e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 711/800\n",
      "learning_rate: 6.087883e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 712/800\n",
      "learning_rate: 6.0776638e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 713/800\n",
      "learning_rate: 6.0674618e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 714/800\n",
      "learning_rate: 6.057277e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 715/800\n",
      "learning_rate: 6.047109e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 716/800\n",
      "learning_rate: 6.0369584e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 717/800\n",
      "learning_rate: 6.0268245e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 718/800\n",
      "learning_rate: 6.016708e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 719/800\n",
      "learning_rate: 6.0066082e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 720/800\n",
      "learning_rate: 5.9965256e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 721/800\n",
      "learning_rate: 5.9864597e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 722/800\n",
      "learning_rate: 5.976411e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 723/800\n",
      "learning_rate: 5.9663787e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 724/800\n",
      "learning_rate: 5.9563637e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 725/800\n",
      "learning_rate: 5.946365e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 726/800\n",
      "learning_rate: 5.9363836e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 727/800\n",
      "learning_rate: 5.9264188e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 728/800\n",
      "learning_rate: 5.9164708e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 729/800\n",
      "learning_rate: 5.906539e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 730/800\n",
      "learning_rate: 5.8966245e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 731/800\n",
      "learning_rate: 5.8867263e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 732/800\n",
      "learning_rate: 5.8768448e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 733/800\n",
      "learning_rate: 5.86698e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 734/800\n",
      "learning_rate: 5.8571317e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0046 - accuracy: 0.9982\n",
      "Epoch 735/800\n",
      "learning_rate: 5.8472997e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 736/800\n",
      "learning_rate: 5.8374844e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 737/800\n",
      "learning_rate: 5.8276855e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 738/800\n",
      "learning_rate: 5.8179034e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0048 - accuracy: 0.9982\n",
      "Epoch 739/800\n",
      "learning_rate: 5.8081372e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 740/800\n",
      "learning_rate: 5.7983878e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 741/800\n",
      "learning_rate: 5.7886544e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 742/800\n",
      "learning_rate: 5.7789377e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 743/800\n",
      "learning_rate: 5.769237e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 744/800\n",
      "learning_rate: 5.7595527e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 745/800\n",
      "learning_rate: 5.7498848e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 746/800\n",
      "learning_rate: 5.740233e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 747/800\n",
      "learning_rate: 5.7305973e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 748/800\n",
      "learning_rate: 5.7209778e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 749/800\n",
      "learning_rate: 5.7113746e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 750/800\n",
      "learning_rate: 5.7017874e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 751/800\n",
      "learning_rate: 5.6922163e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 752/800\n",
      "learning_rate: 5.6826615e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 753/800\n",
      "learning_rate: 5.6731224e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0056 - accuracy: 0.9976\n",
      "Epoch 754/800\n",
      "learning_rate: 5.6635996e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 755/800\n",
      "learning_rate: 5.6540925e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 756/800\n",
      "learning_rate: 5.6446017e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 757/800\n",
      "learning_rate: 5.6351266e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 758/800\n",
      "learning_rate: 5.6256675e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 759/800\n",
      "learning_rate: 5.616224e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 760/800\n",
      "learning_rate: 5.6067965e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 761/800\n",
      "learning_rate: 5.597385e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 762/800\n",
      "learning_rate: 5.5879893e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 763/800\n",
      "learning_rate: 5.578609e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0059 - accuracy: 0.9982\n",
      "Epoch 764/800\n",
      "learning_rate: 5.569245e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 765/800\n",
      "learning_rate: 5.5598965e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 766/800\n",
      "learning_rate: 5.5505636e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 767/800\n",
      "learning_rate: 5.5412464e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 768/800\n",
      "learning_rate: 5.5319448e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 769/800\n",
      "learning_rate: 5.522659e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 770/800\n",
      "learning_rate: 5.5133885e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 771/800\n",
      "learning_rate: 5.5041335e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 772/800\n",
      "learning_rate: 5.494894e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 773/800\n",
      "learning_rate: 5.4856704e-05\n",
      "84/84 [==============================] - 45s 530ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 774/800\n",
      "learning_rate: 5.4764623e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 775/800\n",
      "learning_rate: 5.4672695e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 776/800\n",
      "learning_rate: 5.458092e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 777/800\n",
      "learning_rate: 5.44893e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 778/800\n",
      "learning_rate: 5.4397835e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 779/800\n",
      "learning_rate: 5.430652e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 780/800\n",
      "learning_rate: 5.421536e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 781/800\n",
      "learning_rate: 5.4124357e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0019 - accuracy: 0.9994\n",
      "Epoch 782/800\n",
      "learning_rate: 5.4033502e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 783/800\n",
      "learning_rate: 5.3942804e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0040 - accuracy: 0.9982\n",
      "Epoch 784/800\n",
      "learning_rate: 5.3852254e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 785/800\n",
      "learning_rate: 5.3761858e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 786/800\n",
      "learning_rate: 5.367161e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 787/800\n",
      "learning_rate: 5.358152e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 788/800\n",
      "learning_rate: 5.3491574e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 789/800\n",
      "learning_rate: 5.3401785e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0067 - accuracy: 0.9976\n",
      "Epoch 790/800\n",
      "learning_rate: 5.3312146e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 791/800\n",
      "learning_rate: 5.3222655e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 792/800\n",
      "learning_rate: 5.3133313e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 793/800\n",
      "learning_rate: 5.3044125e-05\n",
      "84/84 [==============================] - 45s 531ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 794/800\n",
      "learning_rate: 5.2955085e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 795/800\n",
      "learning_rate: 5.286619e-05\n",
      "84/84 [==============================] - 45s 534ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 796/800\n",
      "learning_rate: 5.277745e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 797/800\n",
      "learning_rate: 5.2688858e-05\n",
      "84/84 [==============================] - 45s 532ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 798/800\n",
      "learning_rate: 5.2600415e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 799/800\n",
      "learning_rate: 5.251212e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 800/800\n",
      "learning_rate: 5.2423973e-05\n",
      "84/84 [==============================] - 45s 533ms/step - loss: 0.0025 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "epochs = 800\n",
    "data_gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                              vertical_flip=True,\n",
    "                              preprocessing_function = preprocess_input,\n",
    "                              rotation_range = 90)\n",
    "train_generator = data_gen.flow_from_directory(train_path,\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True)\n",
    "history = model.fit_generator(generator=train_generator, epochs=epochs,\n",
    "                              callbacks=[WarmupExponentialDecay(lr_base=0.0002,decay=0.00002,warmup_epochs=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to disk\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "save_folder = 'models/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + dataset\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "# serialize model to JSON\n",
    "#import pickle\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_folder, ModelName + \".json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save(os.path.join(save_folder, ModelName + \".h5\"))\n",
    "#pickle.dump(history.history, open('history/UCMerced_LandUse/AlexNet.p','wb'))\n",
    "print(\"Saved Model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_name):\n",
    "    im=cv2.imread(image_name)\n",
    "    im=cv2.resize(im, (224, 224))\n",
    "    return im\n",
    "\n",
    "#数据扩充\n",
    "def img_Rotation(img,angel):\n",
    "    rows,cols=img.shape[:2]\n",
    "    #90度旋转\n",
    "    M=cv2.getRotationMatrix2D((cols/2,rows/2),angel,1)\n",
    "    dst=cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 420 Test Images\n"
     ]
    }
   ],
   "source": [
    "Extension='.tif'\n",
    "All_Labels = os.listdir(os.path.join(base_path, 'Test'))\n",
    "images = []\n",
    "labels = []\n",
    "outputFileName = []\n",
    "\n",
    "save_folder = 'FeatureMap/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + ModelName + '/' + dataset  +'/'\n",
    "\n",
    "for i in range(0, num_classes):\n",
    "    file_dir = os.path.join(base_path, 'Test', All_Labels[i])\n",
    "    file_names = os.listdir(file_dir)\n",
    "    for file_name in file_names:\n",
    "        if(file_name.endswith(Extension)):\n",
    "            finalFileName = os.path.join(file_dir, file_name)\n",
    "            Label = np.linspace(0, 0, num_classes, dtype='int32')\n",
    "            Label[i] = 1    \n",
    "            images.append(read_image(finalFileName))\n",
    "            labels.append(Label)\n",
    "            outputFileName.append(os.path.join(save_folder, All_Labels[i], file_name).replace(Extension, \".txt\"))\n",
    "\n",
    "print(\"Get %d Test Images\" %(len(images)))\n",
    "output = np.array(images, dtype=\"float32\")\n",
    "output = preprocess_input(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FeatureMap to disk...\n",
      "当前输出：420  \n",
      "保存完成！\n",
      "(420, 512)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from keras.models import Model  \n",
    "OutPutLayer = Model(inputs=model.input, outputs=model.get_layer('fc512').output)\n",
    "print(\"Saved FeatureMap to disk...\")\n",
    "OutputFeatures=[]\n",
    "for i in range(0, len(output)):\n",
    "    p = OutPutLayer.predict(output[i : i + 1])\n",
    "    out=np.reshape(p,p.shape[1])\n",
    "    OutputFeatures.append(out)\n",
    "    print(\"\\r当前输出：%d\" %(i + 1), end= \" \")\n",
    "\n",
    "OutputFeatures = np.array(OutputFeatures, dtype=\"float\") \n",
    "save_folder = 'FeatureMap/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + ModelName + '/' + dataset  +'/'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "np.savetxt(os.path.join(save_folder, dataset + '-Test-2048D.txt'), OutputFeatures)\n",
    "print(\"\\n保存完成！\")\n",
    "print(OutputFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1680 Test Images\n",
      "Saved FeatureMap to disk...\n",
      "当前输出：1680      \n",
      "保存完成！\n",
      "(1680, 512)\n"
     ]
    }
   ],
   "source": [
    "Extension='.tif'\n",
    "All_Labels = os.listdir(os.path.join(base_path, 'Train'))\n",
    "images = []\n",
    "labels = []\n",
    "outputFileName = []\n",
    "\n",
    "save_folder = 'FeatureMap/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + ModelName + '/' + dataset  +'/'\n",
    "\n",
    "for i in range(0, num_classes):\n",
    "    file_dir = os.path.join(base_path, 'Train', All_Labels[i])\n",
    "    file_names = os.listdir(file_dir)\n",
    "    for file_name in file_names:\n",
    "        if(file_name.endswith(Extension)):\n",
    "            finalFileName = os.path.join(file_dir, file_name)\n",
    "            Label = np.linspace(0, 0, num_classes, dtype='int32')\n",
    "            Label[i] = 1    \n",
    "            images.append(read_image(finalFileName))\n",
    "            labels.append(Label)\n",
    "            outputFileName.append(os.path.join(save_folder, All_Labels[i], file_name).replace(Extension, \".txt\"))\n",
    "\n",
    "print(\"Get %d Test Images\" %(len(images)))\n",
    "output = np.array(images, dtype=\"float32\")\n",
    "output = preprocess_input(output)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from keras.models import Model  \n",
    "OutPutLayer = Model(inputs=model.input, outputs=model.get_layer('fc512').output)\n",
    "print(\"Saved FeatureMap to disk...\")\n",
    "OutputFeatures=[]\n",
    "for i in range(0, len(output)):\n",
    "    p = OutPutLayer.predict(output[i : i + 1])\n",
    "    out=np.reshape(p,p.shape[1])\n",
    "    OutputFeatures.append(out)\n",
    "    print(\"\\r当前输出：%d\" %(i + 1), end= \" \")\n",
    "\n",
    "OutputFeatures = np.array(OutputFeatures, dtype=\"float\") \n",
    "save_folder = 'FeatureMap/' + time.strftime(\"%Y-%m-%d\", time.localtime()) + '/' + ModelName + '/' + dataset  +'/'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "np.savetxt(os.path.join(save_folder, dataset + '-Train-2048D.txt'), OutputFeatures)\n",
    "print(\"\\n保存完成！\")\n",
    "print(OutputFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
